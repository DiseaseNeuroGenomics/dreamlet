---
title: "Dreamlet analysis of single cell RNA-seq"
subtitle: 'Linear mixed model analysis of pseudobulk'
author: "Developed by [Gabriel Hoffman](http://gabrielhoffman.github.io/)"
date: "Run on `r Sys.time()`"
documentclass: article
abstract: >
 As the scale of single cell/nucleus RNA-seq has increased, so has the complexity of study designs.  Analysis of datasets with simple study designs can be performed using linear model as in the [muscat package](https://bioconductor.org/packages/muscat).  Yet analysis of datsets with complex study designs such as repeated measures or many technical batches can benefit from linear mixed model analysis to model to correlation structure between samples.  Dreamlet extends the previous work of [muscat](https://www.nature.com/articles/s41467-020-19894-4) to apply linear mixed models to pseudobulk data.  Dreamlet also supports linear models and facilitates application of 1) [variancePartition](https://bioconductor.org/packages/variancePartition) to quantify the contribution of multiple variables to expression variation, and 2) [zenith](https://github.com/GabrielHoffman/zenith) to perform gene set analysis on the differential expression signatures. 
vignette: >
  %\VignetteIndexEntry{Dreamlet analysis of single cell RNA-seq}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\usepackage[utf8]{inputenc}
output:
  html_document:
    toc: true
    toc_float: true
---

<!---

cd /Users/gabrielhoffman/workspace/repos/variancePartition/vignettes

# rm -rf dreamlet_cache/

rmarkdown::render("dreamlet.Rmd")


 devtools::reload("/Users/gabrielhoffman/workspace/repos/dreamlet")

devtools::reload("/Users/gabrielhoffman/workspace/repos/zenith")



--->



```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning=FALSE,
  message=FALSE,
  error = FALSE,
  tidy = FALSE,
  dev = c("png"),
  cache = TRUE)
```

Analysis of PBMCs from 8 individuals stimulated with interferon-Î² [Kang, et al, 2018, Nature Biotech](https://www.nature.com/articles/nbt.4042).

Coming soon: Describe pseudobulk, complex study design, variancePartition, and zenith.

# Process single cell counts
## Preprocess data
```{r preprocess.data}
library(dreamlet)
library(muscat)
library(ExperimentHub)
library(cowplot)
library(zenith)
library(scater)
library(kableExtra)

eh <- ExperimentHub()
sce <- eh[["EH2259"]]
sce <- sce[rowSums(counts(sce) > 0) > 0, ]
sce <- sce[,colData(sce)$multiplets == 'singlet']

qc <- perCellQCMetrics(sce)

# remove cells with few or many detected genes
ol <- isOutlier(metric = qc$detected, nmads = 2, log = TRUE)
sce <- sce[, !ol]

sce <- sce[rowSums(counts(sce) > 1) >= 10, ]
sce <- computeLibraryFactors(sce)
sce <- logNormCounts(sce)
```

## Aggregate to pseudobulk
```{r aggregate}
sce$id <- paste0(sce$stim, sce$ind)
sce <- prepSCE(sce, 
    kid = "cell", # subpopulation assignments
    gid = "stim",  # group IDs (ctrl/stim)
    sid = "id",   # sample IDs (ctrl/stim.1234)
    drop = TRUE)

pb <- aggregateToPseudoBulk(sce,
    assay = "counts", fun = "sum",
    by = c("cluster_id", "sample_id"),
    verbose = FALSE)

# one 'assay' per subpopulation
assayNames(pb)
```

# Voom for pseudobulk 
```{r dreamlet, fig.width=8, fig.height=8}
# Normalize and apply voom
res.proc = processAssays( pb, ~ group_id, min.count=5)
  
plotVoom( res.proc)
```

# Run variancePartition analysis
```{r vp}
vp.lst = fitVarPart( res.proc, ~ group_id)
```

## variancePartition plots
```{r plot.fitVarPart, fig.width=8, fig.height=8}
plotVarPart(vp.lst)
```

# Run dreamlet
```{r test}
# Differential expression analysis within each assay
res.dl = dreamlet( res.proc, ~ group_id)
```

## Volcano plot for each cell type
```{r plotVolcano, fig.width=8, fig.height=8}
plotVolcano( res.dl, coef = 'group_idstim' )
```

## Extract results
```{r extract}
fit = res.dl[["B cells"]]
topTable(fit, coef = 'group_idstim' )
```

## Forrest plot 
```{r forrest}
plotForrest( res.dl, coef = 'group_idstim', gene = 'IRF7')
```



# Gene set analysis using zenith
```{r zenith}
# Load Gene Ontology database 
# use gene SYMBOL
go.gs = get_GeneOntology(to="SYMBOL")
   
# Run zenith gene set analysis on result of dreamlet
res_zenith = zenith_gsa(res.dl, coef = 'group_idstim', go.gs)

# examine results
head(res_zenith)
```

## Heatmap of top genesets
```{r heatmap, fig.height=10, fig.width=5}
# for each cell type select 5 genesets with largest t-statistic
# and 1 geneset with the lowest
plotZenithResults(res_zenith, 5, 1)
```

### All gene sets with FDR < 5%

```{r heatmap2, fig.height=10, fig.width=5}
# get genesets with FDR < 5%
gs = unique(res_zenith$Geneset[res_zenith$FDR < 0.05])

# keep only results of these genesets
df = res_zenith[res_zenith$Geneset %in% gs,]

# plot results, but with no limit based on the highest/lowest t-statistic
plotZenithResults(df, Inf, Inf)
```



# Test differences in cell composition
Compare fraction of each cell type bewteen simulated and unstimulated samples

Plot cell type composition
```{r plotCellComposition, fig.height=5, fig.width=5}
plotCellComposition(pb)     
```

countMatrix = do.call(rbind, int_colData(pb)$n_cells)

res = compositionTest(countMatrix, formula, info=as.data.frame(colData(pb)), useWeights=TRUE)

topTable(res, coef=coef)

vp = compositionVarPart(countMatrix, formula, info=as.data.frame(colData(pb)), useWeights=FALSE)

plotPercentBars(vp)

# get other Diags !!



p = 600
V = Rfast::matrnorm(p,p)

v = rgamma(p, 1, 1)

system.time({
  replicate(1000, sqrt(diag(V %*% tcrossprod(diag(v^2), V))))
  })
 

system.time({
  f = function(){
    # sqrt(diag(V %*% tcrossprod(diag(v^2), V)))
    sqrt(colSums((v^2 * t(V)) * t(V)))
  }
  replicate(1000, f())
  })




diag(V %*% tcrossprod(diag(v^2), V))
emulator::quad.diag(diag(v^2), t(V))


    # colSums(crossprod(M, Conj(x)) * x)

V %*% diag(v^2) %*% t(V)
V %*% (v^2 * t(V))

crossprod( v* t(V))

colSums(crossprod(Diagonal(x=v^2), t(V)) * t(V))


colSums(tcrossprod(Diagonal(x=v^2), V) * t(V))

tcrossprod(Diagonal(x=v^2), V)

y_hat ~ B(y, V^T D^-1 V)

cl = V y 


x = counts
y = ilr(x)

# clr -> ilr
Vx = ilrBase(x = x)
colnames(Vx) = paste0("ilr_", seq_len(ncol(Vx)))
rownames(Vx) = colnames(counts)

# ilr -> clr
Vy = ilrBase(z = y)
colnames(Vy) = paste0("ilr_", seq_len(ncol(Vy)))
rownames(Vy) = colnames(counts)


# test Vx
y[1:3, 1:3]
(clr(x) %*% Vx)[1:3, 1:3]

# test Vy
clr(x)[1:3, 1:3]
(y %*% t(Vy))[1:3, 1:3]

clr(x)[1:3, 1:3]
(Vy %*% t(Vx) %*% clr(x))[1:3, 1:3]

D = ncol(x)
((diag(D) - matrix(1/D, D, D)) %*% clr(x))[1:3, 1:3]
(diag(D) %*% clr(x) - matrix(1/D, D, D) %*% clr(x))[1:3, 1:3]
(diag(D) %*% clr(x))[1:3, 1:3]


y_vars = apply(counts, 1, function(a){
  frac = a / sum(a)
  A = (crossprod(Vx, diag(1/frac)) %*% Vx) / sum(a)
  Sigma = Vy %*% A %*% t(Vy)
  diag(Sigma)
  })


A = Vy %*% ((crossprod(Vx, diag(1/frac)) %*% Vx) / sum(a)) %*% t(Vy)
diag(A)


A = (tcrossprod(Vy,Vx) %*% diag(1/frac) %*% tcrossprod(Vx, Vy)) / sum(a)
diag(A)


H = diag(D) - matrix(1/D, D, D)
A = (H %*% diag(1/frac) %*% H) / sum(a)
diag(A)


H = (diag(D) - matrix(1/D, D, D)) %*% diag(1/sqrt(frac))
A = tcrossprod(H)/ sum(a)
diag(A)

H = diag(D)%*% diag(1/sqrt(frac)) - matrix(1/D, D, D)%*% diag(1/sqrt(frac))
A = tcrossprod(H)/ sum(a)
diag(A)


H = (diag(1/sqrt(frac)) - matrix(1/D, D, D)%*% diag(1/sqrt(frac))) %*% t(diag(1/sqrt(frac)) - matrix(1/D, D, D)%*% diag(1/sqrt(frac)))
A = H/ sum(a)
diag(A)

A = diag(1/sqrt(frac))
B = matrix(1/D, D, D)%*% diag(1/sqrt(frac))
H = (diag(1/sqrt(frac)) - matrix(1/D, D, D)%*% diag(1/sqrt(frac))) %*% t(diag(1/sqrt(frac)) - matrix(1/D, D, D)%*% diag(1/sqrt(frac)))
A = H/ sum(a)
diag(A)













D = ncol(x)
diag(crossprod((diag(1/frac) - matrix(1/D, D, D) %*% diag(1/frac))))/ sum(a)

crossprod((diag(1/sqrt(frac)) - (1/sqrt(frac))/D))/ sum(a)







V = ilrBase(D=ncol(counts))

# Apply ILR transformation
# y = t(ilr(counts))
y = t(log( counts + 0 ) %*% V )
rownames(y) = paste0("ilr_", seq_len(nrow(y)))

y_vars = apply(counts, 1, function(x){
  frac = x / sum(x)
  diag(crossprod(V, diag(1/frac)) %*% V) / sum(x)
  })

fit = lmFit(y, model.matrix(~ group_id, info), weights=1/y_vars)
tcrossprod(coef(fit)[,2], V)



res = compositionTest(counts, ~ group_id, info=info, useWeights=TRUE)
topTable(res, coef=coef)











Perform statistical tests

```{r testCellComposition, eval=FALSE}
formula = ~ group_id
coef = 'group_idstim'

# default is negative binomial model
# poisson model gives similar effect sizes, but *much* smaller standard errors
cellTypeCompositionTest( pb, formula, coef)
```

```{r testCellComposition.eval, results="html", echo=FALSE}
formula = ~ group_id
coef = 'group_idstim'

tab1 = cellTypeCompositionTest( pb, formula, coef)
tab1$pValue = format(tab1$pValue, scientific = TRUE, digits=2)

knitr::kable(tab1, row.names=FALSE, digits=c(2, 2, 2, 3, 3)) %>%  kable_classic(full_width = FALSE, html_font = "Cambria")
```

Partition variation in cell type composition
```{r cellTypeCompositionVarPart, fig.height=2, fig.width=6}
df_vp = cellTypeCompositionVarPart( pb, formula)

rownames(df_vp) = df_vp$assay
plotPercentBars(df_vp[,-1])
```

```{r exit, cache=FALSE, eval=TRUE, echo=FALSE}
knitr::knit_exit()
```



```{r testCellComposition.nb, eval=FALSE}
# negative binomial model is more conservative.
# estimated effects sizes are very similar, 
#   but the standard errors are larger here
cellTypeCompositionTest( pb, formula, coef, "nb")
```

```{r testCellComposition.nb.eval, results="html", echo=FALSE}
tab1 = cellTypeCompositionTest( pb, formula, coef, "nb")
tab1$pValue = format(tab1$pValue, scientific = TRUE, digits=2)

knitr::kable(tab1, row.names=FALSE, digits=c(2, 2, 2, 3, 3)) %>%  kable_classic(full_width = FALSE, html_font = "Cambria")
```
















